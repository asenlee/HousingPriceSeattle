---
title: "Project - Report"
author: "Parham Pishrobat (71097927), Asen Lee (97629497), "
output: pdf_document
---

```{r 0-setup, message=FALSE, warning=FALSE, include=FALSE}
# load required libraries, and set up documents global option
knitr::opts_chunk$set(echo = FALSE)
options(knitr.table.format = "simple")
library(dplyr)
library(tidyr)
library(magrittr)
library(rpart)
library(randomForest)
library(glmnet)
library(mgcv)
library(e1071)
library(quantregForest)
```



```{r 1-clean, message=FALSE, warning=FALSE, include=FALSE}
# (after preliminary visualization), clean the data by filtering out meaningless 
# predictors, removing missing values, factoring categorical predictors, 
# filtering outliers, filtering or balancing unbalanced classes, and making
# a proper transformations
clean_data <- function(train, test) { 
  
  trainPr <- train %>%
    drop_na() %>%
    filter(price < 1e7) %>%
    mutate(zip   = factor(zip_code),
           lot = if_else(lot_size_units == "acre", lot_size*43560, lot_size),
           price  = price/1000) %>%
    filter( lot < 1e5) %>%
    select(zip, beds, baths, size, lot, price)
  
  
  testPr <- test %>%
    drop_na() %>%
    filter(price < 1e7) %>%
    mutate(zip   = factor(zip_code),
           lot = if_else(lot_size_units == "acre", lot_size*43560, lot_size),
           price  = price/1000) %>%
    filter( lot < 1e5) %>%
    select(zip, beds, baths, size, lot, price)
  return(list(train = trainPr, test = testPr))
}
```



```{r 2-summarize, message=FALSE, warning=FALSE, include=FALSE}
# produce many summary statistics and simple plots (some of the results such as correlations are saved in obj)
summarize_data <- function(obj, output = T) { 
  train <- obj$train
  vars  <- colnames(train)
  obj$vars <- vars
  n     <- length(train)
  corr  <- double(n-1)
  print(cor(train[,-1]))
  for (i in 2:(n-1)) {
    corr[i] <- round(cor(train$price, train[, i]), 2)
    if (output) {
      plot(train[, i], train$price, xlab=vars[i], ylab="price")
      plot(train$zip,   train[, i], xlab="zip", ylab=vars[i])
    }
  }
  corr[c(1,6)] <- 1
  obj$corr     <- corr
  return(obj)
}
```



```{r dev}
test    <- read.csv('test.csv')   
housing   <- read.csv('train.csv') %>%
  clean_data(test) %>%
  summarize_data()
```



```{r summary}
names(housing$train)
dim(housing$train)
kableExtra::kable(summary(housing$train))
unique(housing$train$size)
unique(housing$train$lot)
table(housing$train$zip)
```



```{r 3-model, message=FALSE, warning=FALSE, include=FALSE}
# fit models (models are selected based on the results of exploratory analysis on train set)
fit_model <- function(obj) {
  obj$ols   <- lm(price~., data = obj$train)
  #obj$wls  <- lm(price~., data = obj$train, weights = abs(obj$corr))
  obj$rf    <- randomForest(price~., data = obj$train)
  obj$gam   <- gam(price~s(size), family = "gaussian", data = obj$train)
  obj$svm   <- svm(price~., data = obj$train, kernel = "radial", scale = FALSE)
  obj$qRF <- quantregForest(subset(obj$train, select=-c(price)), obj$train$price)
  #obj$ridge <- glmnet()
  #obj$lasso <- glmnet()
  return(obj)
}
```

```{r}
mod <- fit_model(housing)
```



```{r 4-predict, message=FALSE, warning=FALSE, include=FALSE}
# given fitted models and test dataset, compute prediction
predict_price <- function(obj) { 
  obj$qRF <- predict(obj$qRF, what=c(.1, .25, .5, .75, .9), newdata=subset(obj$train, select=-c(price)))

  return(obj)
}

```

```{r}
#' Interval score function for prediction intervals, smaller value is better
#' @description
#' Interval score for prediction intervals
#'
#' @param predobj has 3 (or more) columns: pointprediction, predLB, predUB
#' @param actual corresponding vector of actual values
# (in holdout set, for example)
#' @param level level for prediction interval, e.g., 0.5 or 0.8
#' @return list with
#' summary consisting of level, average length, interval score, coverage rate
#' and
#' imiss with cases where prediction intervals don't contain actual values
#'
intervalScore = function(predObj,actual,level)
{ n = nrow(predObj)
alpha = 1- level
ilow = (actual<predObj[,2]) # overestimation
ihigh = (actual>predObj[,3]) # underestimation
sumlength = sum(predObj[,3]-predObj[,2]) # sum of lengths of prediction intervals
sumlow = sum(predObj[ilow,2]-actual[ilow])*2/alpha
sumhigh = sum(actual[ihigh]-predObj[ihigh,3])*2/alpha
avglength = sumlength/n
IS = (sumlength+sumlow+sumhigh)/n # average length + average under/over penalties
cover = mean(actual>= predObj[,2] & actual<=predObj[,3])
summ = c(level,avglength,IS,cover)
# summary with level, average length, interval score, coverage rate
imiss = which(ilow | ihigh)
list(summary=summ, imiss=imiss)
}
```

```{r }
pred <- predict_price(mod)
IS50qRF <- intervalScore(pred$qRF[,c(3,2,4)], mod$test$price, 0.5)
IS80qRF <- intervalScore(pred$qRF[,c(3,1,5)], mod$test$price, 0.8)
outqRF <- rbind(IS50qRF$summary, IS80qRF$summary)
colnames(outqRF) = c("level", "avgleng", "IS", "cover")
print(outqRF)
```


```{r 5-validate, message=FALSE, warning=FALSE, include=FALSE}
# using all models and holdout predictions, compute diagnostic measures 
#validate_model <- function(obj) { 

# return(obj)
#}
```



```{r 6-select, message=FALSE, warning=FALSE, include=FALSE}
# based on the diagnostics computed in the validation stage, select the best performer
#select_model <- function(obj) { 

# return(obj)
#}
```



```{r main, message=FALSE, warning=FALSE, include=FALSE}
# main <- function(train, test) {
#   
#   analysis   <- train %>%
#     clean_data() %>%                                          # 1. clean
#     summarize_data(output = F) %>%                              # 2. summarize 
#     fit_model() %>%                                             # 3. model
#     predict_price(test) %>%                                     # 4. predict
#     validate_model() %>%                                        # 5. validate
#     select_model() %>%                                          # 6. select
#   
#   return(list(trainR    = analysis$train,
#               trainT    = analysis$trainT,
#               testR     = analysis$test,
#               testT     = analysis$testT,
#               models    = analysis$models,
#               preds50   = analysis$preds50,
#               preds80   = analysis$preds80,
#               best      = analysis$best))
# }
```





##### Data:


##### Precleaning:


 

##### Methods:




##### Results:


```{r run, echo=FALSE, message=FALSE, warning=FALSE}
# test    <- read.csv('test.csv')   
# train   <- read.csv('train.csv') 
# output  <- main(train, test)
```


```{r results, echo=FALSE, message=FALSE, warning=FALSE}

```


##### Interpretation:



```{r plots, echo=FALSE, message=FALSE, warning=FALSE}

```



 